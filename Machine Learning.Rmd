---
title: "Machine Learning"
output:
  html_document: default
  pdf_document: default
date: "2023-02-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r message=FALSE, warning=FALSE}
# Goal: The goal of your project is to predict the manner in which they did the
# exercise. This is the "classe" variable in the training set. You may use any of
# the other variables to predict with. You should create a report describing how
# you built your model, how you used cross validation, what you think the expected
# out of sample error is, and why you made the choices you did. You will also use
# your prediction model to predict 20 different test cases.

# First we need to load the packages we will use.
install.packages("dplyr", repos = "http://cran.us.r-project.org")
library(dplyr)
install.packages("recipes", repos = "http://cran.us.r-project.org")
library(recipes)
install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(randomForest)
install.packages("rattle", repos = "http://cran.us.r-project.org")
library(rattle)
install.packages("naniar", repos = "http://cran.us.r-project.org")
library(naniar)

# Starting off by getting and cleaning the data.
# We want to consider only variables that make sense to contribute to classe.
# Therefore we can remove
  # [1] Variables with 95% results missing
  # [2] Time and Identification variables not contributing to prediction.
  # [3] Variables with near zero variance
  # [4] Highly correlated variables.

testing <- read.csv("C:\\Users\\lombaardl\\Desktop\\Machine Learning\\Course 3 Practical Machine Learning\\Assignments\\pml-testing.csv")
training <- read.csv("C:\\Users\\lombaardl\\Desktop\\Machine Learning\\Course 3 Practical Machine Learning\\Assignments\\pml-training.csv")
dim(training)
# str(training)

# Let us change all missing Character results to na for easier computation.
training <- replace(training,training=="",NA)
testing <- replace(testing,testing=="",NA)

# Setting the seed for reproducability.
set.seed(25)

# Let'us clean the data before using in any particular model.
# We will do the same for training and testing, without looking at the training
# data. We need to remember to only consider the training data once, at the end.

# Remove columns with less than 5% populated data.
training <- training[,(colMeans(is.na(training)))*100 < 95]
testing <- testing[,(colMeans(is.na(testing)))*100 < 95]
dim(training)

# Remove subject/visit information columns as these do not contribute to predictions.
training = subset(training, select = -c(X,user_name,raw_timestamp_part_1,
  raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
testing = subset(testing, select = -c(X,user_name,raw_timestamp_part_1,
  raw_timestamp_part_2,cvtd_timestamp,new_window,num_window))
dim(training)

# We can remove variables with near zero variance.
nearZeroVar(training,saveMetrics=TRUE)
# There seems to be no variables with near zero variance, and so we will not remove
# any from the training or testing set.
dim(training)

# We also do not want to include variables that are highly correlated to avoid
# higher SSE. Lets remove highly correlated variables (high = 0.9 correlation)
HighCOR <- abs(cor(training[,-53]))
diag(HighCOR) <- 0
which(HighCOR > 0.9, arr.ind=T)

RemoveCor = findCorrelation(HighCOR, cutoff=0.9)
RemoveCor = sort(RemoveCor)

# Variables to remove due to high correlation.
RemoveCor
training = training[,-c(RemoveCor)]
dim(training)

# Let's do the same for the testing set.
testing = testing[,-c(RemoveCor)]

# Let's see what we are left with.
names(training)
# Definitely not as much as we started with. :D

# Before modelling we need to convert our predictor to a factor.
training$classe <- as.factor(training$classe)

# Cross validation. We can break training into two sets in order to test our
# results before running on the testing data.
# Create a main training data set (training 1 - 70% of data) and a validation
# set (training2 - 30% of data )
# Training1 will be used to run the models, and training 2 will be used to test
# the aforementioned models. Once testing is done, we can apply to our test sample.
inBuild <- createDataPartition(y=training$classe, p=0.70, list=FALSE)
training1 <- training[inBuild,]
training2 <- training[-inBuild,]
dim(training1)
dim(training2)
 
# We need to confirm that the predictor is not distributed strangely. For example,
# rule out outliers or strong skewness. Since the predictor is categorical data
# we do not have to worry about outliers.
plot(training1$classe, main="Classe distribution", xlab="classe", ylab="Frequency")

# I see no concern within the histogram of Class groupings. No extreme skewness,
# variance or outliers detected. I also do not see any pattern. No pre processing
# required.

# Setting the seed for reproducability.
set.seed(25)

# For each model we will derive predictions based on training2, and calulate the accuracy in order to compare them.

# Model 1: Random forests
# Random forests are considered one of the most accurate machine learning models.
# Although it lacks speed and interpretability. It also tends to overfit. Due to
# the high accuracy I would suggest to try this model first.
mod_rf <- train(classe ~ ., data = training1, method = "rf")
mod_rf
 
predict_rf <- predict(mod_rf,newdata=training2) # Get predictions
confusionMatrix(predict_rf,training2$classe) # Get accuracy

# Model 2: Rpart
# Predicting with trees is usually better performing in a non-linear setting.
# Even though it has tendency of overfitting and variability in results, I still
# want to give it a try.
#mod_rpart <- train(classe ~ ., data=training1, method="rpart")
mod_rpart <- train(classe ~ ., data=training1, method="rpart", tuneLength = 20)
mod_rpart
plot(mod_rpart$finalModel, uniform=TRUE, main="Classification Tree")
text(mod_rpart$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fancyRpartPlot(mod_rpart$finalModel)

predict_rpart <- predict(mod_rpart,newdata=training2) # Get predictions
confusionMatrix(predict_rpart,training2$classe) # Get accuracy

# Model 3: Boosting
# Boosting, takes lots of weak predictors, weight them and add them up, in order
# to get a stronger predictor. Let's see if its worth trying.
# With boostings exceptional timeconsuming run, I decided to leave out the run code
# from the final file. The accuracy was better than rpart but worse than random forest.
# mod_gbm <- train(classe ~ ., data = training1, method = "gbm")
# mod_gbm

# predict_mod_gbm <- predict(mod_gbm,newdata=training2) # Get predictions
# confusionMatrix(predict_mod_gbm,training2$classe) # Get accuracy

# Let's compare accuracies.
# It seems the random forests seems to outperform the other models based on accuracy.
# We will therefore use random forests for the final testing dataframe predictions

# We can now use our best predicting model to predict the 20 values from testing.
# final <- predict(mod_rf,newdata=testing)
# final
# We will not show the final outcome within this presentation. But it will be
# submitted for the quiz.

 

```